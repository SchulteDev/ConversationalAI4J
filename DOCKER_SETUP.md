# Docker Setup Guide - ConversationalAI4J

## ğŸš€ Quick Start (Complete Working System)

With Docker Compose, you can get a complete working ConversationalAI system in one command:

```bash
git clone https://github.com/SchulteDev/ConversationalAI4J.git
cd ConversationalAI4J
docker-compose up
```

**That's it!** ğŸ‰ The system will:
- âœ… Pull Ollama AI service
- âœ… Download llama3.2:1b model automatically  
- âœ… Build and start the demo application
- âœ… Set up proper networking and health checks

## ğŸŒ Access Your AI System

Once all services are up and healthy (takes ~2-3 minutes):

- **Demo Web Interface**: http://localhost:8080
- **Ollama API**: http://localhost:11434

## ğŸ“‹ Service Overview

### ğŸ¤– Ollama Service
- **Purpose**: AI model hosting and processing
- **Port**: 11434
- **Model**: llama3.2:1b (1.2B parameters, ~1.3GB)
- **Health Check**: `/bin/ollama list`

### ğŸ¯ Demo Application  
- **Purpose**: Spring Boot web interface for conversations
- **Port**: 8080  
- **Framework**: Spring Boot 3.5.4 with Thymeleaf
- **Health Check**: `/actuator/health`

### ğŸ”§ Ollama-Init Service
- **Purpose**: One-time model download and setup
- **Behavior**: Pulls llama3.2:1b model then exits
- **Dependencies**: Waits for Ollama to be healthy

## ğŸ› ï¸ Development Commands

### Start Services
```bash
# Start all services in background
docker-compose up -d

# Start with logs visible
docker-compose up

# Rebuild and start (after code changes)
docker-compose up --build
```

### Service Management
```bash
# Check service status
docker-compose ps

# View logs
docker-compose logs demo
docker-compose logs ollama

# Stop services
docker-compose down

# Complete cleanup (removes volumes)
docker-compose down -v
```

### Testing
```bash
# Test demo application
curl http://localhost:8080

# Test Ollama API
curl http://localhost:11434/api/tags

# Send AI message via web interface
curl -X POST -d "message=Hello!" http://localhost:8080/send
```

## ğŸ“Š System Requirements

- **Docker**: Version 20.10+ with Compose V2
- **Memory**: 4GB+ recommended (Ollama model loading)
- **Storage**: ~2GB for model and images
- **Platform**: Windows, macOS, Linux

## ğŸ” Architecture Benefits

### Library Module (Pure API)
- âœ… **No Infrastructure Dependencies**: Just client-side integration code
- âœ… **Framework Agnostic**: Works with any Java application
- âœ… **Simple Integration**: `ConversationalAI.builder().withOllamaModel("llama3.2:1b").build()`

### Demo Module (Complete Infrastructure)
- âœ… **Production Example**: Shows real-world deployment with Docker
- âœ… **Easy Setup**: `docker-compose up` provides fully working system
- âœ… **Best Practices**: Health checks, proper networking, containerization

## ğŸš¨ Troubleshooting

### Service Won't Start
```bash
# Check logs
docker-compose logs [service-name]

# Restart specific service
docker-compose restart [service-name]

# Force rebuild
docker-compose up --build --force-recreate
```

### Model Download Issues
```bash
# Check ollama-init logs
docker-compose logs ollama-init

# Manually pull model
docker exec conversationalai4j-ollama ollama pull llama3.2:1b
```

### Port Conflicts
```bash
# Stop conflicting services
docker ps
docker stop [container-id]

# Or modify ports in docker-compose.yml
ports:
  - "8081:8080"  # Change host port
```

## ğŸ¯ Next Steps

1. **Try the Demo**: Visit http://localhost:8080 and chat with the AI
2. **Explore the API**: Check the ConversationalAI library integration  
3. **Customize**: Modify docker-compose.yml for your needs
4. **Scale**: Add multiple demo instances or different models

---

**ğŸ‰ Congratulations!** You now have a complete, production-ready conversational AI system running with Docker!